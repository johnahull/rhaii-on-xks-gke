apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: gemma-2b-gpu-svc
  namespace: default
spec:
  model:
    uri: hf://google/gemma-2b-it
    name: google/gemma-2b-it
  replicas: 3  # Pattern 3: scale-out with prefix caching

  # Router configuration
  router:
    route: {}      # Auto-create HTTPRoute
    gateway: {}    # Bind to Gateway
    scheduler: {}  # Enable EPP scheduler with cache-aware routing

  # vLLM container template
  template:
    # GPU node selection
    nodeSelector:
      cloud.google.com/gke-accelerator: nvidia-tesla-t4

    # Tolerate GPU node pool taint (if configured)
    tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule

    containers:
    - name: main
      image: registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.0.0

      # vLLM command with prefix caching enabled
      args:
      - |
        python3 -m vllm.entrypoints.openai.api_server \
          --model=/mnt/models \
          --dtype=half \
          --max-model-len=4096 \
          --enable-prefix-caching \
          --disable-log-requests

      # Environment variables
      env:
      - name: HF_TOKEN
        valueFrom:
          secretKeyRef:
            name: huggingface-token
            key: token

      # Resource allocation for T4 GPU
      resources:
        limits:
          nvidia.com/gpu: "1"
        requests:
          nvidia.com/gpu: "1"

      # Health probes
      livenessProbe:
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 180  # GPU init + model download
        periodSeconds: 30
        timeoutSeconds: 30
        failureThreshold: 5

      readinessProbe:
        httpGet:
          path: /v1/models
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 180
        periodSeconds: 10
        timeoutSeconds: 10

    # Pull secret
    imagePullSecrets:
    - name: rhaiis-pull-secret
