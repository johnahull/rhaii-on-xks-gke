# RHAII Deployment on GKE

Deployment guides for **Red Hat AI Inference Services (RHAII)** vLLM workloads on Google Kubernetes Engine (GKE).

## Get Started

### Prerequisites and Setup
- [Prerequisites](docs/prerequisites.md) - Requirements before deploying
- [Environment Setup](docs/environment-setup.md) - Optional: Configure environment variables to streamline commands
- [Operator Installation](docs/operator-installation.md) - Install RHAII operators via [RHAII on XKS](https://github.com/opendatahub-io/rhaii-on-xks)

### Single Replica with Prefix Caching

Single-replica deployment demonstrating vLLM prefix caching effectiveness. Lower cost, simpler configuration.

- **[Simple Demo - TPU](deployments/istio-kserve/simple-caching-demo/deployment-tpu.md)** - 1 TPU node, ~8.3 req/s, ~$15/day
- **[Simple Demo - GPU](deployments/istio-kserve/simple-caching-demo/deployment-gpu.md)** - 1 GPU node, ~6 req/s, ~$12/day
- **[Pattern Overview](deployments/istio-kserve/simple-caching-demo/README.md)** - Architecture and technical details

### 3-Replica Deployment with Cache-Aware Routing

3-replica deployment with cache-aware routing for higher throughput. Requires EPP scheduler fix for cache routing.

- **[3-Replica - TPU](docs/deployment-tpu.md)** - 3 TPU nodes, ~25 req/s, ~$46/day
- **[3-Replica - GPU](docs/deployment-gpu.md)** - 3 GPU nodes, ~18 req/s, ~$36/day

---

## Architecture Overview

```mermaid
graph LR
    Client[Client] -->|HTTP/HTTPS| Gateway[Istio Gateway<br/>LoadBalancer]
    Gateway -->|mTLS| EnvoyFilter[EnvoyFilter<br/>ext_proc]
    EnvoyFilter -->|Request body| EPP[EPP Scheduler<br/>Cache-aware routing]
    EPP -->|Route by prefix hash| VLLM1[vLLM Replica 1<br/>GPU/TPU]
    EPP -->|Route by prefix hash| VLLM2[vLLM Replica 2<br/>GPU/TPU]
    EPP -->|Route by prefix hash| VLLM3[vLLM Replica 3<br/>GPU/TPU]

    VLLM1 -.->|Cache hit: 60-75% faster| Client
    VLLM2 -.->|Cache hit: 60-75% faster| Client
    VLLM3 -.->|Cache hit: 60-75% faster| Client

    style EPP fill:#ffe6cc
    style VLLM1 fill:#e1f5ff
    style VLLM2 fill:#e1f5ff
    style VLLM3 fill:#e1f5ff
```

**Key Features:**
- Cache-aware routing - Identical prefixes route to same replica for cache hits
- 60-75% latency reduction - Cached prefix processing is dramatically faster
- mTLS encryption - Secure service-to-service communication
- Load balancing - Smart routing balances cache affinity and replica load

---

## ğŸ“– Documentation

**Complete Index:** [Customer Guides Hub](docs/README.md)

### Operations
- [Verification & Testing](docs/verification-testing.md) - Validate your deployment
- [Troubleshooting](docs/troubleshooting.md) - Common issues and solutions

---

## ğŸ› ï¸ Automation Scripts

All deployment guides use these automation scripts in `scripts/`:

### Validation Scripts
- `preflight-check.sh` - Comprehensive prerequisite validation
- `check-accelerator-availability.sh` - Zone and accelerator validation
- `check-nodepool-prerequisites.sh` - Node pool compatibility and quota checks

### Deployment Scripts
- `create-gke-cluster.sh` - Automated cluster creation with integrated validation
- `delete-gke-cluster.sh` - Safe cluster deletion or scale-to-zero
- `verify-deployment.sh` - Post-deployment health checks

**Example:**
```bash
# Run validation
./scripts/preflight-check.sh --customer --accelerator tpu

# Create cluster
./scripts/create-gke-cluster.sh --tpu

# Verify deployment
./scripts/verify-deployment.sh
```

---

## ğŸ“‚ Repository Structure

```
rhaii-on-xks-gke/
â”œâ”€â”€ README.md                              # This file
â”‚
â”œâ”€â”€ docs/                                  # Customer guides
â”‚   â”œâ”€â”€ README.md                          # Guide index
â”‚   â”œâ”€â”€ deployment-tpu.md                  # TPU deployment guide
â”‚   â”œâ”€â”€ deployment-gpu.md                  # GPU deployment guide
â”‚   â”œâ”€â”€ prerequisites.md                   # Setup requirements
â”‚   â”œâ”€â”€ environment-setup.md               # Environment variable configuration
â”‚   â”œâ”€â”€ operator-installation.md           # RHAII operator installation
â”‚   â”œâ”€â”€ verification-testing.md            # Validation procedures
â”‚   â”œâ”€â”€ prefix-caching-verification.md     # Prefix caching configuration verification
â”‚   â””â”€â”€ troubleshooting.md                 # Common issues
â”‚
â”œâ”€â”€ scripts/                               # Automation scripts
â”‚   â”œâ”€â”€ create-gke-cluster.sh              # Cluster creation
â”‚   â”œâ”€â”€ delete-gke-cluster.sh              # Cluster deletion / scale-to-zero
â”‚   â”œâ”€â”€ verify-deployment.sh               # Post-deployment validation
â”‚   â”œâ”€â”€ test-cache-routing.sh              # Cache routing and throughput test
â”‚   â”œâ”€â”€ preflight-check.sh                 # Prerequisite validation
â”‚   â”œâ”€â”€ check-accelerator-availability.sh  # Zone validation
â”‚   â””â”€â”€ check-nodepool-prerequisites.sh    # Node pool validation
â”‚
â”œâ”€â”€ deployments/                           # Kubernetes manifests
â”‚   â”œâ”€â”€ gpu-operator/
â”‚   â”‚   â””â”€â”€ resourcequota-gcp-critical-pods.yaml
â”‚   â””â”€â”€ istio-kserve/
â”‚       â”œâ”€â”€ simple-caching-demo/           # Quick demo (single replica)
â”‚       â”‚   â”œâ”€â”€ README.md
â”‚       â”‚   â”œâ”€â”€ deployment-tpu.md
â”‚       â”‚   â”œâ”€â”€ deployment-gpu.md
â”‚       â”‚   â”œâ”€â”€ namespace-rhaii-inference.yaml
â”‚       â”‚   â”œâ”€â”€ llmisvc-tpu-single-replica.yaml
â”‚       â”‚   â”œâ”€â”€ llmisvc-gpu-single-replica.yaml
â”‚       â”‚   â””â”€â”€ httproute-health-models.yaml
â”‚       â””â”€â”€ caching-pattern/               # 3-replica deployment
â”‚           â””â”€â”€ manifests/
â”‚               â”œâ”€â”€ llmisvc-tpu-caching.yaml
â”‚               â”œâ”€â”€ llmisvc-gpu-caching.yaml
â”‚               â”œâ”€â”€ envoyfilter-epp-mtls-fix.yaml
â”‚               â”œâ”€â”€ envoyfilter-epp-mtls-fix-tpu.yaml
â”‚               â”œâ”€â”€ envoyfilter-ext-proc-gpu.yaml
â”‚               â”œâ”€â”€ envoyfilter-ext-proc-tpu.yaml
â”‚               â”œâ”€â”€ envoyfilter-route-extproc-body.yaml
â”‚               â”œâ”€â”€ istio-cni.yaml
â”‚               â””â”€â”€ networkpolicies/
â”‚                   â”œâ”€â”€ allow-gateway-to-vllm.yaml
â”‚                   â”œâ”€â”€ allow-epp-scheduler.yaml
â”‚                   â”œâ”€â”€ allow-istio.yaml
â”‚                   â””â”€â”€ allow-vllm-egress.yaml
â”‚
â”œâ”€â”€ templates/                             # Secret templates
â”‚   â”œâ”€â”€ redhat-pull.yaml.template          # Red Hat registry credentials template
â”‚   â””â”€â”€ huggingface-token.yaml.template    # HuggingFace token template
â”‚
â””â”€â”€ env.sh.example                         # Environment variable example
```

---

## ğŸš€ Deployment Overview

### Step-by-Step Process

1. **Prerequisites** (15-30 minutes, one-time)
   - Install tools, configure accounts, request quotas

2. **Cluster Creation** (~20 minutes)
   ```bash
   ./scripts/create-gke-cluster.sh --tpu  # or --gpu
   ```

3. **Operator Installation** (~10 minutes)
   - Clone [RHAII on XKS](https://github.com/opendatahub-io/rhaii-on-xks) repository
   - Deploy operators (cert-manager, Istio, KServe, LWS)

4. **Deploy Workload** (~12 minutes)
   ```bash
   # Simple demo (single replica)
   kubectl apply -f deployments/istio-kserve/simple-caching-demo/llmisvc-tpu-single-replica.yaml

   # 3-replica deployment
   kubectl apply -f deployments/istio-kserve/caching-pattern/manifests/llmisvc-tpu-caching.yaml
   ```

5. **Verify & Test** (~5 minutes)
   ```bash
   ./scripts/verify-deployment.sh
   ./scripts/test-cache-routing.sh
   ```

**Total time:** ~45-50 minutes for complete deployment

---

## ğŸ†˜ Getting Help

1. Review [Troubleshooting](docs/troubleshooting.md) - Solutions to common issues
2. Run verification: `./scripts/verify-deployment.sh --operators-only`
3. Check logs: `kubectl logs -l serving.kserve.io/inferenceservice`

---

## ğŸ“ License

This repository provides deployment configurations and documentation for Red Hat AI Inference Services (RHAII) on Google Cloud Platform.

**External Dependencies:**
- [RHAII on XKS](https://github.com/opendatahub-io/rhaii-on-xks) - Operator installation (required)
- Red Hat AI Inference Services - Commercial product (requires license)

---

## ğŸ”— Related Resources

- [RHAII on XKS GitHub](https://github.com/opendatahub-io/rhaii-on-xks) - Official operator repository
- [llm-d Documentation](https://llm-d.ai/docs/) - LLM framework architecture
- [GKE AI Labs](https://gke-ai-labs.dev) - Google Cloud AI resources
- [KServe Documentation](https://kserve.github.io/website/) - KServe reference
