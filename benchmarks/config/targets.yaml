# Deployment target configurations for LLM inference benchmarks
# Supports: vLLM, OpenAI-compatible APIs, Ollama, LM Studio, and more
#
# To add a new target:
# 1. Add a new entry under 'targets' with a unique name
# 2. Specify base_url (endpoint URL without /v1/completions)
# 3. Specify model name (as expected by the API)
# 4. Configure max_tokens and other parameters as needed

targets:
  # === vLLM Deployments ===
  gke-t4:
    name: "GKE NVIDIA T4 (vLLM)"
    base_url: "http://136.116.159.221:8000"
    model: "google/gemma-2b-it"
    max_tokens: 4096
    accelerator: "T4 GPU"
    gpu_memory: "13.12 GiB"
    kv_cache: "6.08 GiB"
    backend: "vLLM + XFormers"
    expected_concurrency: 86
    zone: "us-central1-a"
    # Multiple models supported (change deployment to test different models)
    supported_models:
      - "google/gemma-2b-it"
      - "microsoft/Phi-3-mini-4k-instruct"
      - "mistralai/Mistral-7B-Instruct-v0.3"
      - "meta-llama/Llama-3.1-8B-Instruct"

  llm-d-pattern2-gpu:
    name: "llm-d Pattern 2 GPU Multi-Model (2x T4)"
    base_url: "http://35.209.92.117"  # llm-d Pattern 2 Gateway IP
    model: "microsoft/Phi-3-mini-4k-instruct"
    max_tokens: 2048
    accelerator: "2x NVIDIA T4 GPU"
    gpu_memory: "14.58 GiB per GPU"
    backend: "vLLM + XFormers"
    replicas: 2  # 1 replica per model
    prefix_caching: false
    gpu_memory_utilization: 0.85
    routing: "Unified Scheduler (model-based routing with dynamic discovery)"
    expected_concurrency: 40
    zone: "us-central1-a"
    notes: "Pattern 2 Multi-Model deployment with unified EPP scheduler"
    supported_models:
      - "microsoft/Phi-3-mini-4k-instruct"  # Pattern 2 native model
      - "google/gemma-2b-it"  # Pattern 1 model via unified routing

  llm-d-pattern3-gpu:
    name: "llm-d Pattern 3 GPU (3x T4)"
    base_url: "http://35.208.175.15"  # llm-d Gateway IP
    model: "Qwen/Qwen2.5-3B-Instruct"
    max_tokens: 2048
    accelerator: "3x NVIDIA T4 GPU"
    gpu_memory: "16 GiB per GPU"
    backend: "vLLM + FLASHINFER"
    replicas: 3
    prefix_caching: true
    gpu_memory_utilization: 0.75
    routing: "Intelligent (prefix-cache-scorer weight 3.0)"
    expected_concurrency: 120
    zone: "us-central1-a"
    notes: "Pattern 3 N/S-Caching Scale-Out with intelligent routing"
    supported_models:
      - "Qwen/Qwen2.5-3B-Instruct"

  tpu-v6e:
    name: "TPU v6e (vLLM)"
    base_url: "http://35.214.154.17"  # llm-d Gateway IP (OLD)
    model: "Qwen/Qwen2.5-3B-Instruct"
    max_tokens: 2048
    accelerator: "TPU v6e (4 chips)"
    backend: "vLLM + JAX/XLA"
    expected_concurrency: 50
    zone: "europe-west4-a"
    notes: "XLA precompilation during startup (~151s), fast inference after Ready"
    # Multiple models supported (require redeployment to change)
    supported_models:
      - "Qwen/Qwen2.5-3B-Instruct"
      - "microsoft/Phi-3-mini-4k-instruct"
      - "mistralai/Mistral-7B-Instruct-v0.3"
      - "google/gemma-2-9b-it"

  istio-tpu-pattern1:
    name: "Pattern 1 Istio + KServe + TPU v6e"
    base_url: "http://34.6.79.145/llm-d-inference-scheduling/qwen2-3b-pattern1"
    model: "/mnt/models"
    max_tokens: 2048
    accelerator: "TPU v6e-4 (4 chips, 2x2 topology)"
    backend: "vLLM + JAX/XLA"
    replicas: 1
    prefix_caching: false
    routing: "Istio Gateway → EPP Scheduler → vLLM (ext_proc with BUFFERED bodies)"
    expected_concurrency: 7
    zone: "europe-west4-a"
    cluster: "llmd-istio-tpu-pattern1"
    notes: "Istio + KServe Helm deployment with HTTPS Gateway, NetworkPolicies enforced, mTLS PERMISSIVE"
    infrastructure:
      - "Istio v1.26.6 (sail-operator)"
      - "KServe v3.4.0-ea.1 (Helm chart)"
      - "cert-manager (self-signed TLS)"
      - "NetworkPolicies (default-deny)"
    supported_models:
      - "Qwen/Qwen2.5-3B-Instruct"

  llm-d-pattern3-tpu:
    name: "llm-d Pattern 3 TPU (3x TPU v6e)"
    base_url: "http://35.214.223.251"  # llm-d Pattern 3 Gateway IP
    model: "Qwen/Qwen2.5-3B-Instruct"
    max_tokens: 2048
    accelerator: "3x TPU v6e (12 chips total)"
    backend: "vLLM + JAX/XLA"
    replicas: 3
    prefix_caching: true
    routing: "Intelligent (prefix-cache-scorer weight 3.0)"
    expected_concurrency: 150
    zone: "europe-west4-a"
    notes: "Pattern 3 N/S-Caching Scale-Out with intelligent routing on TPU"
    supported_models:
      - "Qwen/Qwen2.5-3B-Instruct"

  # === Local LLM Servers ===
  ollama-local:
    name: "Ollama (Local)"
    base_url: "http://localhost:11434"
    model: "llama3.2:3b"  # Change to any Ollama model
    max_tokens: 2048
    backend: "Ollama"
    notes: "Requires Ollama running locally. Install from https://ollama.ai"

  lmstudio-local:
    name: "LM Studio (Local)"
    base_url: "http://localhost:1234"
    model: "local-model"  # LM Studio uses the loaded model name
    max_tokens: 4096
    backend: "LM Studio"
    notes: "Requires LM Studio with a model loaded and server started"

  llamacpp-local:
    name: "llama.cpp Server (Local)"
    base_url: "http://localhost:8080"
    model: "gpt-3.5-turbo"  # llama.cpp accepts any name
    max_tokens: 2048
    backend: "llama.cpp"
    notes: "Requires llama.cpp server running with --port 8080"

  # === Cloud API Providers (for comparison) ===
  # Note: These require API keys and will incur costs
  openai-gpt4:
    name: "OpenAI GPT-4 Turbo"
    base_url: "https://api.openai.com"
    model: "gpt-4-turbo-preview"
    max_tokens: 4096
    backend: "OpenAI API"
    notes: "Requires OPENAI_API_KEY environment variable. Costs apply."
    api_key_env: "OPENAI_API_KEY"

  openai-gpt35:
    name: "OpenAI GPT-3.5 Turbo"
    base_url: "https://api.openai.com"
    model: "gpt-3.5-turbo"
    max_tokens: 4096
    backend: "OpenAI API"
    notes: "Requires OPENAI_API_KEY environment variable. Costs apply."
    api_key_env: "OPENAI_API_KEY"

  # === Example: Custom Deployment ===
  # Uncomment and customize for your deployment
  # my-deployment:
  #   name: "My Custom vLLM Deployment"
  #   base_url: "http://my-server.example.com:8000"
  #   model: "meta-llama/Llama-3.1-8B-Instruct"
  #   max_tokens: 8192
  #   accelerator: "A100 GPU"
  #   backend: "vLLM"
  #   notes: "Custom deployment description"

# Default test parameters applied to all targets
defaults:
  timeout: 300  # Request timeout in seconds
  temperature: 0.7
  top_p: 0.9
  stream: false
  connect_timeout: 30
  read_timeout: 300
